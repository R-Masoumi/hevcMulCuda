
#include <iostream>
#include <cstdlib>
#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <stdio.h>
static int X265_DEPTH = 8;
#define ALIGN_VAR_32(T, var) __declspec(align(32)) T var
typedef short int16_t;
using namespace std;
enum Type {D1,D2,ID1,ID2};

__device__ static signed int g_at[4][4] =
{ 
	{ 29, 55, 74, 84 },
	{ 74, 74, 0, -74 },
	{ 84, -29, -74, 55 },
	{ 55, -84, 74, -29 } 
};

__device__ const int16_t g_t4[4][4] =
{
	{ 64, 64, 64, 64 },
	{ 83, 36, -36, -83 },
	{ 64, -64, -64, 64 },
	{ 36, -83, 83, -36 }
};

__device__ const int16_t g_t8[8][8] =
{
	{ 64, 64, 64, 64, 64, 64, 64, 64 },
	{ 89, 75, 50, 18, -18, -50, -75, -89 },
	{ 83, 36, -36, -83, -83, -36, 36, 83 },
	{ 75, -18, -89, -50, 50, 89, 18, -75 },
	{ 64, -64, -64, 64, 64, -64, -64, 64 },
	{ 50, -89, 18, 75, -75, -18, 89, -50 },
	{ 36, -83, 83, -36, -36, 83, -83, 36 },
	{ 18, -50, 75, -89, 89, -75, 50, -18 }
};

__device__ const int16_t g_t16[16][16] =
{
	{ 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64 },
	{ 90, 87, 80, 70, 57, 43, 25, 9, -9, -25, -43, -57, -70, -80, -87, -90 },
	{ 89, 75, 50, 18, -18, -50, -75, -89, -89, -75, -50, -18, 18, 50, 75, 89 },
	{ 87, 57, 9, -43, -80, -90, -70, -25, 25, 70, 90, 80, 43, -9, -57, -87 },
	{ 83, 36, -36, -83, -83, -36, 36, 83, 83, 36, -36, -83, -83, -36, 36, 83 },
	{ 80, 9, -70, -87, -25, 57, 90, 43, -43, -90, -57, 25, 87, 70, -9, -80 },
	{ 75, -18, -89, -50, 50, 89, 18, -75, -75, 18, 89, 50, -50, -89, -18, 75 },
	{ 70, -43, -87, 9, 90, 25, -80, -57, 57, 80, -25, -90, -9, 87, 43, -70 },
	{ 64, -64, -64, 64, 64, -64, -64, 64, 64, -64, -64, 64, 64, -64, -64, 64 },
	{ 57, -80, -25, 90, -9, -87, 43, 70, -70, -43, 87, 9, -90, 25, 80, -57 },
	{ 50, -89, 18, 75, -75, -18, 89, -50, -50, 89, -18, -75, 75, 18, -89, 50 },
	{ 43, -90, 57, 25, -87, 70, 9, -80, 80, -9, -70, 87, -25, -57, 90, -43 },
	{ 36, -83, 83, -36, -36, 83, -83, 36, 36, -83, 83, -36, -36, 83, -83, 36 },
	{ 25, -70, 90, -80, 43, 9, -57, 87, -87, 57, -9, -43, 80, -90, 70, -25 },
	{ 18, -50, 75, -89, 89, -75, 50, -18, -18, 50, -75, 89, -89, 75, -50, 18 },
	{ 9, -25, 43, -57, 70, -80, 87, -90, 90, -87, 80, -70, 57, -43, 25, -9 }
};

__device__ const int16_t g_t32[32][32] =
{
	{ 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64 },
	{ 90, 90, 88, 85, 82, 78, 73, 67, 61, 54, 46, 38, 31, 22, 13, 4, -4, -13, -22, -31, -38, -46, -54, -61, -67, -73, -78, -82, -85, -88, -90, -90 },
	{ 90, 87, 80, 70, 57, 43, 25, 9, -9, -25, -43, -57, -70, -80, -87, -90, -90, -87, -80, -70, -57, -43, -25, -9, 9, 25, 43, 57, 70, 80, 87, 90 },
	{ 90, 82, 67, 46, 22, -4, -31, -54, -73, -85, -90, -88, -78, -61, -38, -13, 13, 38, 61, 78, 88, 90, 85, 73, 54, 31, 4, -22, -46, -67, -82, -90 },
	{ 89, 75, 50, 18, -18, -50, -75, -89, -89, -75, -50, -18, 18, 50, 75, 89, 89, 75, 50, 18, -18, -50, -75, -89, -89, -75, -50, -18, 18, 50, 75, 89 },
	{ 88, 67, 31, -13, -54, -82, -90, -78, -46, -4, 38, 73, 90, 85, 61, 22, -22, -61, -85, -90, -73, -38, 4, 46, 78, 90, 82, 54, 13, -31, -67, -88 },
	{ 87, 57, 9, -43, -80, -90, -70, -25, 25, 70, 90, 80, 43, -9, -57, -87, -87, -57, -9, 43, 80, 90, 70, 25, -25, -70, -90, -80, -43, 9, 57, 87 },
	{ 85, 46, -13, -67, -90, -73, -22, 38, 82, 88, 54, -4, -61, -90, -78, -31, 31, 78, 90, 61, 4, -54, -88, -82, -38, 22, 73, 90, 67, 13, -46, -85 },
	{ 83, 36, -36, -83, -83, -36, 36, 83, 83, 36, -36, -83, -83, -36, 36, 83, 83, 36, -36, -83, -83, -36, 36, 83, 83, 36, -36, -83, -83, -36, 36, 83 },
	{ 82, 22, -54, -90, -61, 13, 78, 85, 31, -46, -90, -67, 4, 73, 88, 38, -38, -88, -73, -4, 67, 90, 46, -31, -85, -78, -13, 61, 90, 54, -22, -82 },
	{ 80, 9, -70, -87, -25, 57, 90, 43, -43, -90, -57, 25, 87, 70, -9, -80, -80, -9, 70, 87, 25, -57, -90, -43, 43, 90, 57, -25, -87, -70, 9, 80 },
	{ 78, -4, -82, -73, 13, 85, 67, -22, -88, -61, 31, 90, 54, -38, -90, -46, 46, 90, 38, -54, -90, -31, 61, 88, 22, -67, -85, -13, 73, 82, 4, -78 },
	{ 75, -18, -89, -50, 50, 89, 18, -75, -75, 18, 89, 50, -50, -89, -18, 75, 75, -18, -89, -50, 50, 89, 18, -75, -75, 18, 89, 50, -50, -89, -18, 75 },
	{ 73, -31, -90, -22, 78, 67, -38, -90, -13, 82, 61, -46, -88, -4, 85, 54, -54, -85, 4, 88, 46, -61, -82, 13, 90, 38, -67, -78, 22, 90, 31, -73 },
	{ 70, -43, -87, 9, 90, 25, -80, -57, 57, 80, -25, -90, -9, 87, 43, -70, -70, 43, 87, -9, -90, -25, 80, 57, -57, -80, 25, 90, 9, -87, -43, 70 },
	{ 67, -54, -78, 38, 85, -22, -90, 4, 90, 13, -88, -31, 82, 46, -73, -61, 61, 73, -46, -82, 31, 88, -13, -90, -4, 90, 22, -85, -38, 78, 54, -67 },
	{ 64, -64, -64, 64, 64, -64, -64, 64, 64, -64, -64, 64, 64, -64, -64, 64, 64, -64, -64, 64, 64, -64, -64, 64, 64, -64, -64, 64, 64, -64, -64, 64 },
	{ 61, -73, -46, 82, 31, -88, -13, 90, -4, -90, 22, 85, -38, -78, 54, 67, -67, -54, 78, 38, -85, -22, 90, 4, -90, 13, 88, -31, -82, 46, 73, -61 },
	{ 57, -80, -25, 90, -9, -87, 43, 70, -70, -43, 87, 9, -90, 25, 80, -57, -57, 80, 25, -90, 9, 87, -43, -70, 70, 43, -87, -9, 90, -25, -80, 57 },
	{ 54, -85, -4, 88, -46, -61, 82, 13, -90, 38, 67, -78, -22, 90, -31, -73, 73, 31, -90, 22, 78, -67, -38, 90, -13, -82, 61, 46, -88, 4, 85, -54 },
	{ 50, -89, 18, 75, -75, -18, 89, -50, -50, 89, -18, -75, 75, 18, -89, 50, 50, -89, 18, 75, -75, -18, 89, -50, -50, 89, -18, -75, 75, 18, -89, 50 },
	{ 46, -90, 38, 54, -90, 31, 61, -88, 22, 67, -85, 13, 73, -82, 4, 78, -78, -4, 82, -73, -13, 85, -67, -22, 88, -61, -31, 90, -54, -38, 90, -46 },
	{ 43, -90, 57, 25, -87, 70, 9, -80, 80, -9, -70, 87, -25, -57, 90, -43, -43, 90, -57, -25, 87, -70, -9, 80, -80, 9, 70, -87, 25, 57, -90, 43 },
	{ 38, -88, 73, -4, -67, 90, -46, -31, 85, -78, 13, 61, -90, 54, 22, -82, 82, -22, -54, 90, -61, -13, 78, -85, 31, 46, -90, 67, 4, -73, 88, -38 },
	{ 36, -83, 83, -36, -36, 83, -83, 36, 36, -83, 83, -36, -36, 83, -83, 36, 36, -83, 83, -36, -36, 83, -83, 36, 36, -83, 83, -36, -36, 83, -83, 36 },
	{ 31, -78, 90, -61, 4, 54, -88, 82, -38, -22, 73, -90, 67, -13, -46, 85, -85, 46, 13, -67, 90, -73, 22, 38, -82, 88, -54, -4, 61, -90, 78, -31 },
	{ 25, -70, 90, -80, 43, 9, -57, 87, -87, 57, -9, -43, 80, -90, 70, -25, -25, 70, -90, 80, -43, -9, 57, -87, 87, -57, 9, 43, -80, 90, -70, 25 },
	{ 22, -61, 85, -90, 73, -38, -4, 46, -78, 90, -82, 54, -13, -31, 67, -88, 88, -67, 31, 13, -54, 82, -90, 78, -46, 4, 38, -73, 90, -85, 61, -22 },
	{ 18, -50, 75, -89, 89, -75, 50, -18, -18, 50, -75, 89, -89, 75, -50, 18, 18, -50, 75, -89, 89, -75, 50, -18, -18, 50, -75, 89, -89, 75, -50, 18 },
	{ 13, -38, 61, -78, 88, -90, 85, -73, 54, -31, 4, 22, -46, 67, -82, 90, -90, 82, -67, 46, -22, -4, 31, -54, 73, -85, 90, -88, 78, -61, 38, -13 },
	{ 9, -25, 43, -57, 70, -80, 87, -90, 90, -87, 80, -70, 57, -43, 25, -9, -9, 25, -43, 57, -70, 80, -87, 90, -90, 87, -80, 70, -57, 43, -25, 9 },
	{ 4, -13, 22, -31, 38, -46, 54, -61, 67, -73, 78, -82, 85, -88, 90, -90, 90, -90, 88, -85, 82, -78, 73, -67, 61, -54, 46, -38, 31, -22, 13, -4 }
};

// Multiply the arrays A and B on GPU and save the result in C
// C(m,n) = A(m,k) * B(k,n)
__global__ void transform(const int16_t *src, int16_t *dst, int n, const int shift, Type type) {
	int row = blockIdx.y * blockDim.y + threadIdx.y;
	int col = blockIdx.x * blockDim.x + threadIdx.x;
	extern __shared__ int16_t shared[];
	shared[row * n + col] = src[row * n + col];
	switch (n)
	{
	case 4:
		shared[n*n + row*n + col] = g_t4[row][col];
		break;
	case 8:
		shared[n*n + row*n + col] = g_t8[row][col];
		break;
	case 16:
		shared[n*n + row*n + col] = g_t16[row][col];
		break;
	case 32:
		shared[n*n + row*n + col] = g_t32[row][col];
		break;
	default:
		shared[n*n + row*n + col] = g_at[row][col];
		break;
	}
	__syncthreads();
	int sum = 0;
	for (int i = 0; i < n; i++)
	{
		switch (type)
		{
		case D1:
			sum += shared[n*n + row*n + i] * shared[i * n + col];
			break;
		case D2:
			sum += shared[row * n + i] * shared[n*n + col*n + i];
			break;
		case ID1:
			sum += shared[n*n + i*n + row] * shared[i * n + col];
			break;
		case ID2:
			sum += shared[row * n + i] * shared[n*n + i*n + col];
			break;
		default:
			break;
		}
	}
	sum >>= shift;
	dst[row * n + col] = sum;
}

//Print matrix A(nr_rows_A, nr_cols_A) storage in column-major format
void print_matrix(const int16_t *A, int nr_rows_A, int nr_cols_A) {

	for (int i = 0; i < nr_rows_A; ++i){
		for (int j = 0; j < nr_cols_A; ++j){
			std::cout << A[j * nr_rows_A + i] << " ";
		}
		std::cout << std::endl;
	}
	std::cout << std::endl;
}

int main() {
	// Allocate 3 arrays on CPU
	const int n = 4;
	int i,j;

	ALIGN_VAR_32(int16_t, h_A[n * n]);
	ALIGN_VAR_32(int16_t, h_B[n * n]);
	int16_t init = -256;
	for (i = 0; i < n; ++i)
	{
		for (j = 0; j < n; ++j)
		{
			h_A[i * n + j] = init;
		}
	}

	int cudaStatus = cudaSetDevice(0);
	if (cudaStatus != cudaSuccess) {
		fprintf(stderr, "cudaSetDevice failed!  Do you have a CUDA-capable GPU installed?");
		return 0;
	}
	// Allocate 3 arrays on GPU
	int16_t *dst, *src, *src2;
	cudaMalloc(&src, n * n * sizeof(int16_t));
	cudaMalloc(&src2, n * n * sizeof(int16_t));
	cudaMalloc(&dst, n * n * sizeof(int16_t));

	// If you already have useful values in A and B you can copy them in GPU:
	cudaMemcpy(src, h_A, n * n * sizeof(int16_t), cudaMemcpyHostToDevice);

	// Fill the arrays A and B on GPU with random numbers
	//GPU_fill_rand(d_A, nr_rows_A, nr_cols_A);
	//GPU_fill_rand(d_B, nr_rows_B, nr_cols_B);

	// Optionally we can copy the data back on CPU and print the arrays
	cudaMemcpy(h_A, src, n * n * sizeof(int16_t), cudaMemcpyDeviceToHost);
	std::cout << "src =" << std::endl;
	print_matrix(h_A, n, n);
	// Launch kernel
	const int shift_1st = 1 + X265_DEPTH - 8;
	const int shift_2nd = 8;

	dim3 dimGrid(1, 1);
	dim3  dimBlock(n, n);
	transform << <dimGrid, dimBlock, 2 * n * n * sizeof(int16_t) >> >(src, src2, n, shift_1st, D1);
	cudaDeviceSynchronize();
	transform << <dimGrid, dimBlock, 2 * n * n * sizeof(int16_t) >> >(src2, dst, n, shift_2nd, D2);

	// Copy (and print) the result on host memory
	cudaMemcpy(h_B, dst, n * n * sizeof(int16_t), cudaMemcpyDeviceToHost);
	std::cout << "dst =" << std::endl;
	print_matrix(h_B, n, n);

	const int ishift_1st = 7;
	const int ishift_2nd = 12 - (X265_DEPTH - 8);
	cudaDeviceSynchronize();
	transform << <dimGrid, dimBlock,2* n * n * sizeof(int16_t) >> >(dst, src2, n, ishift_1st, ID1);
	cudaDeviceSynchronize();
	transform << <dimGrid, dimBlock, 2 * n * n * sizeof(int16_t) >> >(src2, dst, n, ishift_2nd, ID2);
	// Copy (and print) the result on host memory
	cudaMemcpy(h_B, dst, n * n * sizeof(int16_t), cudaMemcpyDeviceToHost);
	std::cout << "dst =" << std::endl;
	print_matrix(h_B, n, n);

	//Free GPU memory
	cudaFree(src);
	cudaFree(src2);
	cudaFree(dst);

	// Free CPU memory
	free(h_A);
 	free(h_B);

	return 0;
}